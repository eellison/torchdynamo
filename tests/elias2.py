import torch

# def forward(seq_len, word_pieces_lengths, word_pieces, attn_masks):
#     _set_grad_enabled = torch._C._set_grad_enabled(False)
#     long_tensor = torch.LongTensor([654, 278, 1071, 1581, 2087, 3056, 1376, 35, 3090, 265, 2897, 952, 1589, 2725, 1578, 1441, 1262, 115, 1731, 1115, 2920, 1706, 2424, 638, 116, 1499, 1523, 1498, 183, 1143, 1142, 1077, 1368, 2084, 1517, 2863, 501, 2358, 1950, 438, 2233, 2926, 479, 2686, 1567, 575, 839, 529, 1049, 933, 948, 2635, 2856, 1789, 1662, 1524, 174, 199, 0, 1119, 168, 1078, 2779, 1324, 1616, 3033, 694, 2270, 2422, 2439, 1035, 617, 6, 2941, 1456, 1947, 2235, 2000, 893, 2930, 3073, 1761, 148, 2375, 1447, 1629, 175, 3065, 1470, 621, 1008, 1991, 2422, 3015, 2967, 1807, 1878, 2166, 155, 588, 2053, 2696, 748, 552, 1423, 1022, 414, 2421, 132, 1936, 828, 966, 2381, 2453, 700, 835, 1097, 1937, 348, 2226, 2236, 1918, 803, 310, 2873, 2515, 561, 1389, 354, 1215, 3100, 1363, 56, 1997, 2659, 464, 1532, 2708, 1151, 319, 2252, 1000, 364, 1226, 1655, 1992, 2395, 2260, 2178, 1354, 1954, 1673, 1892, 2852, 1271, 1480, 2707, 3024, 2519, 2142, 419, 123, 587, 36, 2724, 2148, 2159, 970, 2747, 947, 2185, 271, 159, 187, 2134, 365, 481, 313, 1826, 3098, 2884, 2050, 42, 2299, 1369, 1247, 2815, 424, 92, 2800, 2184, 289, 347, 2304, 1324, 1081, 1890, 2115, 2355, 153, 2577, 2183, 2476, 2100, 1128, 901, 1478, 149, 2129, 338, 1805, 2677, 1695, 1579, 791, 1619, 794, 2685, 1184, 2690, 940, 2589, 2994, 1966, 1955, 2552, 623, 1186, 68, 2416, 2905, 33, 1334, 1510, 2059, 1806, 2331, 1146, 2953, 2511, 636, 574, 466, 1400, 2572, 2451, 244, 1733, 1641, 2897, 1842, 2224, 1445, 188, 1399, 2851, 1529, 677, 3035, 428, 2695, 1233, 562, 1866, 784, 1972, 1415, 763, 1339, 2742, 2493, 2555, 2017, 1278, 200, 772, 2371, 2302, 315, 385, 1947, 1833, 2492, 2506, 761, 502, 218, 494, 1237, 1001, 366, 599, 779, 1372, 1741, 2690, 637, 918, 1837, 1133, 1005, 1452, 1249, 2719, 2293, 2137, 297, 2318, 88, 1134, 1660, 279, 225, 1002, 2727, 2262, 394, 2557, 1867, 2328, 2344, 938, 852, 1583, 1957, 1887, 1204, 2079, 562, 1348, 1358, 2125, 3053, 1205, 748, 2203, 562, 2864, 600, 1431, 2070, 297, 234, 2472, 171, 98, 2991, 75, 1877, 2781, 2199, 553, 2144, 2892, 861, 514, 2322, 1349, 1526, 465, 512, 710, 1500, 1195, 1115, 431, 2888, 415, 1361, 578, 1492, 587, 177, 2062, 972, 954, 3022, 638, 2988, 349, 1880, 2948, 478, 2461, 2756, 1797, 723, 3089, 2895, 2146, 31, 1558, 488, 2578, 2570, 2341, 283, 2099, 707, 28, 1318, 2734, 274, 2317, 850, 756, 1236, 1164, 2242, 1064, 1257, 2026, 935, 3029, 1539, 1485, 1907, 1235, 2663, 1240, 2398, 2337, 3055, 511, 569, 1637, 2916, 1491, 1191, 2071, 227, 1791, 1422, 1231, 711, 1701, 1160, 405, 680, 2457, 1827, 1044, 1905, 302, 2495, 1103, 317, 2255, 2430, 447, 3040, 2752, 3012, 2818, 887, 2616, 753, 2335, 690, 1309, 2965, 1010, 702, 1160, 1859, 1426, 1471, 508, 2909, 1001, 2289, 2402, 425, 840])
#     getitem = word_pieces_lengths[0]
#     add = torch.add(getitem, 1);  getitem = None
#     word_pieces[(0, slice(1, add, None))] = long_tensor;  setitem = word_pieces;  word_pieces = add = long_tensor = None
#     getitem_1 = word_pieces_lengths[0];  word_pieces_lengths = None
#     add_1 = torch.add(getitem_1, 2);  getitem_1 = None
#     getitem_2 = attn_masks[(0, slice(None, add_1, None))];  attn_masks = add_1 = None
#     fill_ = getitem_2.fill_(1);  getitem_2 = None
#     getitem_3 = seq_len[1];  seq_len = None
#     _set_grad_enabled_1 = torch._C._set_grad_enabled(True)
#     return (getitem_3,)

# seq_len = torch.zeros([6], dtype=torch.int64)
# word_pieces_lengths = torch.zeros([6], dtype=torch.int64)
# word_pieces = torch.zeros([6, 476], dtype=torch.int64)
# attn_masks = torch.zeros([6, 476], dtype=torch.int64)
# forward(seq_len, word_pieces_lengths, word_pieces, attn_masks)

import torch
import torchdynamo
from typing import List

def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    # print("my_compiler() called with FX graph:")
    # gm.graph.print_tabular()
    return gm.forward  # return a python callable


@torchdynamo.optimize(my_compiler)
def f(x, y):
    return x[0, y:]

f(torch.randn(5, 5), torch.tensor(3))
f(torch.randn(5, 5), torch.tensor(3))

# fx_g = make_fx(f)(torch.randn(5, 5), torch.tensor(3))
# print(fx_g.code)
